{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset directory and all files in it\n",
    "dbutils.fs.rm(\"FileStore/tables\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /---------------------------------------------------------------\n",
    "# Course            : Big Data Analytics\n",
    "# Course Code       : CDB3034\n",
    "# Assignment        : 2\n",
    "# Group             : 1\n",
    "# Student Name 1    : Chan Seow Fen / 0207368\n",
    "# Student Name 2    : Cheah Pin Chee / 0197637\n",
    "# Student Name 3    : Ong Yi Wen / 0207333\n",
    "# Student Name 4    : Saw Keat Loon / 0207778\n",
    "# /---------------------------------------------------------------\n",
    "# Data source: https://openlearning.uowmkdu.edu.my/courses/pg-cbd-3034n-big-data-analysis-jjoshua/data_a2/?cl=1\n",
    "# Original Data Files: T_Data_C1.csv, T_Data_C2.csv, T_Data_C3.csv\n",
    "# /---------------------------------------------------------------\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Task 1: Data Loading and Data Cleaning\n",
    "# Make sure all the 3 files are loaded into the Spark environment!\n",
    "# 1.1 Renaming the files and remove original files\n",
    "dbutils.fs.mv(\"FileStore/tables/T_Data_C1.csv\", \"/FileStore/tables/BDA_T_data_C1.csv\")\n",
    "dbutils.fs.mv(\"FileStore/tables/T_Data_C2.csv\", \"/FileStore/tables/BDA_T_data_C2.csv\")\n",
    "dbutils.fs.mv(\"FileStore/tables/T_Data_C3.csv\", \"/FileStore/tables/BDA_T_data_C3.csv\")\n",
    "dbutils.fs.rm(\"FileStore/tables/T_Data_Cl.csv\")\n",
    "dbutils.fs.rm(\"FileStore/tables/T_Data_C2.csv\")\n",
    "dbutils.fs.rm(\"FileStore/tables/T_Data_C3.csv\")\n",
    "\n",
    "# 1.2 List all files in the directory\n",
    "display(dbutils.fs.ls(\"/FileStore/tables/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Load the data from the files\n",
    "df_c1 = spark.read.csv(\"dbfs:/FileStore/tables/BDA_T_data_C1.csv\", header=True, inferSchema=True)\n",
    "df_c2 = spark.read.csv(\"dbfs:/FileStore/tables/BDA_T_data_C2.csv\", header=True, inferSchema=True)\n",
    "df_c3 = spark.read.csv(\"dbfs:/FileStore/tables/BDA_T_data_C3.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.2 Display the data from the files\n",
    "print(\"Data from BDA_T_data_C1.csv\")\n",
    "print(\"Data size of BDA_T_data_C1.csv: \", df_c1.count(), \" rows\" , \" and \", len(df_c1.columns), \" columns\")\n",
    "display(df_c1)\n",
    "print(\"Data from BDA_T_data_C2.csv\")\n",
    "print(\"Data size of BDA_T_data_C2.csv: \", df_c2.count(), \" rows\" , \" and \", len(df_c2.columns), \" columns\")\n",
    "display(df_c2)\n",
    "print(\"Data from BDA_T_data_C3.csv\")\n",
    "print(\"Data size of BDA_T_data_C3.csv: \", df_c3.count(), \" rows\" , \" and \", len(df_c3.columns), \" columns\")\n",
    "display(df_c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Cleaning df_c1\n",
    "print(\"Before cleaning for df_c1\")\n",
    "df_c1.display()  # Before cleaning\n",
    "\n",
    "# 3.1.1 Rename the columns\n",
    "df_c1_cleaned = df_c1  # Copy the original dataframe to a new dataframe\n",
    "df_c1_cleaned = (\n",
    "    df_c1_cleaned.withColumnRenamed(\"c01\", \"customerID\")\n",
    "    .withColumnRenamed(\"c02\", \"gender\")\n",
    "    .withColumnRenamed(\"c03\", \"SeniorCitizen\")\n",
    "    .withColumnRenamed(\"c04\", \"Partner\")\n",
    "    .withColumnRenamed(\"c05\", \"Dependents\")\n",
    "    .withColumnRenamed(\"c06\", \"tenure\")\n",
    "    .withColumnRenamed(\"c07\", \"PhoneServices\")\n",
    "    .withColumnRenamed(\"c08\", \"MultipleLines\")\n",
    "    .withColumnRenamed(\"c09\", \"InternetServices\")\n",
    "    .withColumnRenamed(\"c10\", \"OnlineSecurity\")\n",
    "    .withColumnRenamed(\"c11\", \"OnlineBackup\")\n",
    "    .withColumnRenamed(\"c12\", \"DeviceProtection\")\n",
    "    .withColumnRenamed(\"c13\", \"TechSupport\")\n",
    "    .withColumnRenamed(\"c14\", \"StreamingTV\")\n",
    "    .withColumnRenamed(\"c15\", \"StreamingMovies\")\n",
    "    .withColumnRenamed(\"c16\", \"Contract\")\n",
    "    .withColumnRenamed(\"c17\", \"PaperlessBilling\")\n",
    "    .withColumnRenamed(\"c18\", \"PaymentMethod\")\n",
    "    .withColumnRenamed(\"c19\", \"MonthlyCharges\")\n",
    "    .withColumnRenamed(\"c20\", \"TotalCharges\")\n",
    "    .withColumnRenamed(\"c21\", \"Churn\")\n",
    ")\n",
    "\n",
    "# 3.1.2 Change the data type\n",
    "df_c1_cleaned = (\n",
    "    df_c1_cleaned.withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast(IntegerType()))\n",
    "    .withColumn(\"tenure\", col(\"tenure\").cast(IntegerType()))\n",
    "    .withColumn(\"MonthlyCharges\", col(\"MonthlyCharges\").cast(DoubleType()))\n",
    "    .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# 3.1.3 Remove rows with missing values\n",
    "df_c1_cleaned = df_c1_cleaned.dropna()\n",
    "\n",
    "print(\"After cleaning for df_c1\")\n",
    "df_c1_cleaned.display()  # After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Cleaning df_c2\n",
    "print(\"Before cleaning for df_c2\")\n",
    "df_c2.display()  # Before cleaning\n",
    "# 3.2.1 Amending Abnormal Data\n",
    "df_c2_cleaned = df_c2  # Copy the original dataframe to a new dataframe\n",
    "# Col 4 Replace '0' with 'No' in the 'Partner' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"Partner\", when((df_c2_cleaned[\"Partner\"] == '0'), 'No').otherwise(df_c2_cleaned[\"Partner\"]))\n",
    "# Col 5 Replace '0' with 'No' in the 'Dependents' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"Dependents\", when((df_c2_cleaned[\"Dependents\"] == '0'), 'No').otherwise(df_c2_cleaned[\"Dependents\"]))\n",
    "# Col 7 Replace '0' with 'No' in the 'PhoneService' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"PhoneService\", when((df_c2_cleaned[\"PhoneService\"] == '0'), 'No').otherwise(df_c2_cleaned[\"PhoneService\"]))\n",
    "# Col 8 Replace '0phone service' with 'No phone service' in the 'MultipleLines' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"MultipleLines\", when((df_c2_cleaned[\"MultipleLines\"] == '0phone service'), 'No phone service').otherwise(df_c2_cleaned[\"MultipleLines\"]))\n",
    "# Col 8 Replace '0' with 'No' in the 'MultipleLines' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"MultipleLines\", when((df_c2_cleaned[\"MultipleLines\"] == '0'), 'No').otherwise(df_c2_cleaned[\"MultipleLines\"]))\n",
    "# Col 9 Replace '0' with 'No' in the 'InternetService' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"InternetService\", when((df_c2_cleaned[\"InternetService\"] == '0'), 'No').otherwise(df_c2_cleaned[\"InternetService\"]))\n",
    "# Col 10 Replace '0' with 'No' in the 'OnlineSecurity' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"OnlineSecurity\", when((df_c2_cleaned[\"OnlineSecurity\"] == '0'), 'No').otherwise(df_c2_cleaned[\"OnlineSecurity\"]))\n",
    "# Col 10 Replace 'NAN' with 'No internet service' in the 'OnlineSecurity' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"OnlineSecurity\", when((df_c2_cleaned[\"OnlineSecurity\"] == 'NAN'), 'No internet service').otherwise(df_c2_cleaned[\"OnlineSecurity\"]))\n",
    "# Col 11 Replace '0' with 'No' in the 'OnlineBackup' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"OnlineBackup\", when((df_c2_cleaned[\"OnlineBackup\"] == '0'), 'No').otherwise(df_c2_cleaned[\"OnlineBackup\"]))\n",
    "# Col 11 Replace 'NAN' with 'No internet service' in the 'OnlineBackup' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"OnlineBackup\", when((df_c2_cleaned[\"OnlineBackup\"] == 'NAN'), 'No internet service').otherwise(df_c2_cleaned[\"OnlineBackup\"]))\n",
    "# Col 12 Replace '0' with 'No' in the 'DeviceProtection' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"DeviceProtection\", when((df_c2_cleaned[\"DeviceProtection\"] == '0'), 'No').otherwise(df_c2_cleaned[\"DeviceProtection\"]))\n",
    "# Col 12 Replace 'NAN' with 'No internet service' in the 'DeviceProtection' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"DeviceProtection\", when((df_c2_cleaned[\"DeviceProtection\"] == 'NAN'), 'No internet service').otherwise(df_c2_cleaned[\"DeviceProtection\"]))\n",
    "# Col 13 Replace '0' with 'No' in the 'TechSupport' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"TechSupport\", when((df_c2_cleaned[\"TechSupport\"] == '0'), 'No').otherwise(df_c2_cleaned[\"TechSupport\"]))\n",
    "# Col 13 Replace 'NAN' with 'No internet service' in the 'TechSupport' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"TechSupport\", when((df_c2_cleaned[\"TechSupport\"] == 'NAN'), 'No internet service').otherwise(df_c2_cleaned[\"TechSupport\"]))\n",
    "# Col 14 Replace '0' with 'No' in the 'StreamingTV' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"StreamingTV\", when((df_c2_cleaned[\"StreamingTV\"] == '0'), 'No').otherwise(df_c2_cleaned[\"StreamingTV\"]))\n",
    "# Col 14 Replace 'NAN' with 'No internet service' in the 'StreamingTV' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"StreamingTV\", when((df_c2_cleaned[\"StreamingTV\"] == 'NAN'), 'No internet service').otherwise(df_c2_cleaned[\"StreamingTV\"]))\n",
    "# Col 15 Replace '0' with 'No' in the 'StreamingMovies' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"StreamingMovies\", when((df_c2_cleaned[\"StreamingMovies\"] == '0'), 'No').otherwise(df_c2_cleaned[\"StreamingMovies\"]))\n",
    "# Col 15 Replace 'NAN' with 'No internet service' in the 'StreamingMovies' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"StreamingMovies\", when((df_c2_cleaned[\"StreamingMovies\"] == 'NAN'), 'No internet service').otherwise(df_c2_cleaned[\"StreamingMovies\"]))\n",
    "# Col 17 Replace '0' with 'No' in the 'PaperlessBilling' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"PaperlessBilling\", when((df_c2_cleaned[\"PaperlessBilling\"] == '0'), 'No').otherwise(df_c2_cleaned[\"PaperlessBilling\"]))\n",
    "# Col 21 Replace '0' with 'No' in the 'churn' column\n",
    "df_c2_cleaned = df_c2_cleaned.withColumn(\"churn\", when((df_c2_cleaned[\"churn\"] == '0'), 'No').otherwise(df_c2_cleaned[\"churn\"]))\n",
    "\n",
    "# 3.2.2 Change the data type\n",
    "df_c2_cleaned = (\n",
    "    df_c2_cleaned.withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast(IntegerType()))\n",
    "    .withColumn(\"tenure\", col(\"tenure\").cast(IntegerType()))\n",
    "    .withColumn(\"MonthlyCharges\", col(\"MonthlyCharges\").cast(DoubleType()))\n",
    "    .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# 3.2.3 Remove rows with missing values\n",
    "df_c2_cleaned = df_c2_cleaned.dropna()\n",
    "\n",
    "print(\"After cleaning for df_c2\")\n",
    "df_c2_cleaned.display()  # After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Cleaning df_c3\n",
    "df_c3.display()  # Before cleaning\n",
    "# 3.3.1 Amending Abnormal Data\n",
    "df_c3_cleaned = df_c3  # Copy the original dataframe to a new dataframe\n",
    "# Col 9 Replace 'no' with 'No' in the 'InternetService' column\n",
    "df_c3_cleaned = df_c3_cleaned.withColumn(\"InternetService\", when((df_c3_cleaned[\"InternetService\"] == 'no'), 'No').otherwise(df_c3_cleaned[\"InternetService\"]))\n",
    "\n",
    "# 3.3.2 Change the data type\n",
    "df_c3_cleaned = (\n",
    "    df_c3_cleaned.withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast(IntegerType()))\n",
    "    .withColumn(\"tenure\", col(\"tenure\").cast(IntegerType()))\n",
    "    .withColumn(\"MonthlyCharges\", col(\"MonthlyCharges\").cast(DoubleType()))\n",
    "    .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# 3.3.3 Remove rows with missing values\n",
    "df_c3_cleaned = df_c3_cleaned.dropna()\n",
    "\n",
    "print(\"After cleaning for df_c3\")\n",
    "df_c3_cleaned.display()  # After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Merging the data\n",
    "# 4.1 Union df_c1_cleaned, df_c2_cleaned, and df_c3_cleaned\n",
    "df_merged = df_c1_cleaned.union(df_c2_cleaned).union(df_c3_cleaned).distinct()\n",
    "\n",
    "# Show the size of the merged data\n",
    "print(\"Merged data\")\n",
    "print(\"Data size of merged data: \", df_merged.count(), \" rows\" , \" and \", len(df_merged.columns), \" columns\")\n",
    "display(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Saving the merged data\n",
    "# Coallesce the data into 1 partition and save it as a csv file\n",
    "df_merged.coalesce(1).repartition(1).write.mode(\"overwrite\").format(\"csv\").option(\"header\", \"true\").save(\"dbfs:/FileStore/tables/Final\")\n",
    "\n",
    "# 5.2 List all files in the directory\n",
    "display(dbutils.fs.ls(\"/FileStore/tables/Final/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Move and Load the final data\n",
    "# Replace the csv file name and path according to generated csv file in previous step\n",
    "dbutils.fs.mv(\"dbfs:/FileStore/tables/Final/part-00000-tid-2861974613874961528-5a08c8c2-d540-41a7-9f10-b40a6c93f71d-470-1-c000.csv\", \"/FileStore/tables/BDA_T_data_Final.csv\")\n",
    "dbutils.fs.rm(\"FileStore/tables/Final\", True)\n",
    "display(dbutils.fs.ls(\"/FileStore/tables/\"))\n",
    "\n",
    "# Downloading the file\n",
    "# Reference for community edition: https://stackoverflow.com/questions/49019706/databricks-download-a-dbfs-filestore-file-to-my-local-machine\n",
    "# Replace the part after ?o= with the id from the community session\n",
    "# https://community.cloud.databricks.com/files/tables/BDA_T_data_Final.csv?o="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Load the final data\n",
    "df_final = spark.read.csv(\"dbfs:/FileStore/tables/BDA_T_data_Final.csv\", header=True, inferSchema=True)\n",
    "print(\"Data from BDA_T_data_Merged.csv\")\n",
    "print(\"Data size of BDA_T_data_Merged.csv: \", df_final.count(), \" rows\", \" and \", len(df_final.columns), \" columns\")\n",
    "display(df_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
